{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4966dae2-9f92-48c5-8918-6c59971822ba",
   "metadata": {},
   "source": [
    "https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b6b8e",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e9faaf-7dea-4597-834a-bde60fc6612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb_path\n",
      "  Downloading ipynb_path-0.1.5-py3-none-any.whl (2.9 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from ipynb_path) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.0->ipynb_path) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.0->ipynb_path) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.0->ipynb_path) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0,>=2.0->ipynb_path) (2023.7.22)\n",
      "Installing collected packages: ipynb_path\n",
      "Successfully installed ipynb_path-0.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.4.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.59.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.35.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.6/479.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.59.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, keras, grpcio, absl-py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.32.0\n",
      "    Uninstalling grpcio-1.32.0:\n",
      "      Successfully uninstalled grpcio-1.32.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.35.0\n",
      "    Uninstalling google-auth-1.35.0:\n",
      "      Successfully uninstalled google-auth-1.35.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.0\n",
      "    Uninstalling tensorboard-2.4.0:\n",
      "      Successfully uninstalled tensorboard-2.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.4.0\n",
      "    Uninstalling tensorflow-2.4.0:\n",
      "      Successfully uninstalled tensorflow-2.4.0\n",
      "Successfully installed absl-py-2.0.0 flatbuffers-23.5.26 google-auth-2.23.4 google-auth-oauthlib-1.0.0 grpcio-1.59.2 keras-2.13.1 libclang-16.0.6 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting typing-extensions==4.1.1\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 22.12.1 requires cupy-cuda11x, which is not installed.\n",
      "cudf 22.12.1 requires numba>=0.56.2, but you have numba 0.48.0 which is incompatible.\n",
      "librosa 0.10.1 requires numba>=0.51.0, but you have numba 0.48.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numba==0.49\n",
      "  Downloading numba-0.49.0-cp38-cp38-manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: llvmlite<=0.33.0.dev0,>=0.31.0.dev0 in /opt/conda/lib/python3.8/site-packages (from numba==0.49) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from numba==0.49) (1.24.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba==0.49) (68.0.0)\n",
      "Installing collected packages: numba\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.48.0\n",
      "    Uninstalling numba-0.48.0:\n",
      "      Successfully uninstalled numba-0.48.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 22.12.1 requires cupy-cuda11x, which is not installed.\n",
      "cudf 22.12.1 requires numba>=0.56.2, but you have numba 0.49.0 which is incompatible.\n",
      "librosa 0.10.1 requires numba>=0.51.0, but you have numba 0.49.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numba-0.49.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy==1.24.3 in /opt/conda/lib/python3.8/site-packages (1.24.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numba==0.56.2\n",
      "  Downloading numba-0.56.2-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.2)\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.24,>=1.18 (from numba==0.56.2)\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools<60 (from numba==0.56.2)\n",
      "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 kB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.8/site-packages (from numba==0.56.2) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata->numba==0.56.2) (3.11.0)\n",
      "Installing collected packages: setuptools, numpy, llvmlite, numba\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.31.0\n",
      "    Uninstalling llvmlite-0.31.0:\n",
      "      Successfully uninstalled llvmlite-0.31.0\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.49.0\n",
      "    Uninstalling numba-0.49.0:\n",
      "      Successfully uninstalled numba-0.49.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 22.12.1 requires cupy-cuda11x, which is not installed.\n",
      "ipywidgets 8.1.1 requires comm>=0.1.3, but you have comm 0.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed llvmlite-0.39.1 numba-0.56.2 numpy-1.23.5 setuptools-59.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting comm==0.1.3\n",
      "  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: traitlets>=5.3 in /opt/conda/lib/python3.8/site-packages (from comm==0.1.3) (5.13.0)\n",
      "Installing collected packages: comm\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "Successfully installed comm-0.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting cupy-cuda11x\n",
      "  Downloading cupy_cuda11x-12.2.0-cp38-cp38-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.20 in /opt/conda/lib/python3.8/site-packages (from cupy-cuda11x) (1.23.5)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.8/site-packages (from cupy-cuda11x) (0.8.2)\n",
      "Downloading cupy_cuda11x-12.2.0-cp38-cp38-manylinux2014_x86_64.whl (92.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mmm\n",
      "\u001b[?25hInstalling collected packages: cupy-cuda11x\n",
      "Successfully installed cupy-cuda11x-12.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install ipynb_path\n",
    "# !pip install --upgrade tensorflow\n",
    "# !pip install typing-extensions==4.1.1\n",
    "# !pip install numba==0.49\n",
    "# !pip install numpy==1.24.3\n",
    "# !pip install numba==0.56.2\n",
    "# !pip install comm==0.1.3\n",
    "# !pip install cupy-cuda11x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656acfde",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e60f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import gc  \n",
    "from albumentations import (\n",
    "    Compose, Normalize, Resize,\n",
    "    RandomResizedCrop, HorizontalFlip,\n",
    "    RandomBrightnessContrast, ShiftScaleRotate\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import ipynb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de489ddb-ed45-435f-b6ad-bbad443c0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_try\n"
     ]
    }
   ],
   "source": [
    "# ノートブック名を取得\n",
    "__file__ = ipynb_path.get()\n",
    "notebook_name = __file__.split('/')[-1].split('.')[0]\n",
    "\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bbf73d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    patience=20\n",
    "    num_epochs=20\n",
    "    mixup_epochs=0\n",
    "    batch_size=24\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd592c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Use a chosen seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1fc9498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set PyTorch to use GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce585db4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231114215330\n"
     ]
    }
   ],
   "source": [
    "# Get current date and time in JST\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "current_datetime = datetime.now(jst)\n",
    "# Get current date and time\n",
    "formatted_datetime = current_datetime.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "print(formatted_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71301958",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d474b9",
   "metadata": {},
   "source": [
    "## def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b83035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def view_img(dir_name, file_name):\n",
    "    \n",
    "    info_table = pd.read_csv(f'../data/{dir_name}.csv', header=None, names=['name', 'label'])\n",
    "    target_label = info_table[info_table['name']==f'{file_name}']['label']\n",
    "    \n",
    "    # # Display the image\n",
    "    img = Image.open(f'../data/{dir_name}/'+file_name)\n",
    "    # img = Image.open('../data/train_1/'+f'train_1811.png')\n",
    "    \n",
    "    plt.title(file_name)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Label:\", target_label.values[0])  # values[0] is used to get the first value if multiple rows match the condition\n",
    "    print(\"Size of the image:\", img.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd918cf",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376029bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/train_1.csv', header=None, names=['name', 'label'])\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4337c399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random_idx = random.choice(range(len(df)))\n",
    "# file_name = f'train_{random_idx}.png'\n",
    "# dir_name = 'train_1'\n",
    "\n",
    "# view_img(dir_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705e13c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Directory to search\n",
    "# directory = \"../data/train_1/\"\n",
    "\n",
    "# # Maximum file size in bytes (40K in this case)\n",
    "# max_size = 40 * 1024\n",
    "\n",
    "# # Get list of files in the directory and subdirectories\n",
    "# file_list = []\n",
    "\n",
    "# for directory in [\"../data/train_1/\",\"../data/train_2/\",\"../data/train_3/\",\"../data/train_4/\"]:\n",
    "#     for foldername, subfolders, filenames in os.walk(directory):\n",
    "#         for filename in filenames:\n",
    "#             full_path = os.path.join(foldername, filename)\n",
    "#             if os.path.getsize(full_path) <= max_size:\n",
    "#                 file_list.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5df6ad25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e08d688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_name = f'train_1474.png'\n",
    "# dir_name = 'train_3'\n",
    "\n",
    "# view_img(dir_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a407131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get image paths for label 0 and 1\n",
    "# image_paths_0 = df[df['label'] == 0]['name'].values\n",
    "# image_paths_1 = df[df['label'] == 1]['name'].values\n",
    "\n",
    "# # Function to display images\n",
    "# def display_images(image_paths, title):\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     for i in range(9):  # display 9 images\n",
    "#         plt.subplot(3,3,i+1)\n",
    "#         img = Image.open(image_paths[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(title)\n",
    "#         plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Display images for label 0 and 1\n",
    "# display_images('../data/train_1/'+ image_paths_0, 'label_0')\n",
    "# display_images('../data/train_1/'+ image_paths_1, 'label_1')\n",
    "\n",
    "# ## 画像が欠損しているデータもあり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d422cefc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.groupby('label')['name'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e1668",
   "metadata": {},
   "source": [
    "# ベースモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0606e",
   "metadata": {},
   "source": [
    "## 学習フェーズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0add230",
   "metadata": {},
   "source": [
    "### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d2afa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    transforms_dict = {\n",
    "        'train': Compose([\n",
    "            RandomResizedCrop(512, 512),\n",
    "            HorizontalFlip(),\n",
    "            RandomBrightnessContrast(),\n",
    "            ShiftScaleRotate(),\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "        'valid': Compose([\n",
    "            Resize(512, 512),\n",
    "            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ]),\n",
    "    }\n",
    "    return transforms_dict\n",
    "\n",
    "\n",
    "def load_datasets(df, root_dir, transforms_dict):\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    datasets = {\n",
    "        'train': CustomImageDataset(train_df, root_dir, transform=transforms_dict['train']),\n",
    "        'valid': CustomImageDataset(valid_df, root_dir, transform=transforms_dict['valid'])\n",
    "    }\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_dataloaders(datasets, batch_size):\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(datasets['train'], batch_size=batch_size, shuffle=True),\n",
    "        'valid': DataLoader(datasets['valid'], batch_size=batch_size, shuffle=False)\n",
    "    }\n",
    "    return dataloaders\n",
    "\n",
    "    \n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)  # Use OpenCV to read the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)  # Apply the transformations\n",
    "            image = transformed[\"image\"]\n",
    "        return image, label\n",
    "    \n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "          \n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "    # val_loss_minの値を取得\n",
    "    def get_value(self):\n",
    "        return self.val_loss_min\n",
    "        \n",
    "def train_model_mixup_el_stop(dataloaders, model, criterion, optimizer, num_epochs, file_name, mixup_epochs, patience):\n",
    "    early_stopping = EarlyStopping(patience=patience, path=f'../models/{formatted_datetime}_{file_name}.pth',verbose=True)\n",
    "    stop_training = False  # 早期停止フラグ\n",
    "\n",
    "    logs = []\n",
    "    train_loss = []\n",
    "    train_auc = []\n",
    "    val_loss = []\n",
    "    val_auc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Mixupを適用する条件\n",
    "                    # if phase == 'train' and epoch < mixup_epochs:\n",
    "                    #     inputs, labels_a, labels_b, lmd = mixup_data(inputs, labels)\n",
    "                    #     outputs = model(inputs)\n",
    "                    #     loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lmd)\n",
    "                    #     _, preds = torch.max(outputs, 1)\n",
    "                    #     # Mixupでの正確さの計算\n",
    "                    #     correct_predictions += lmd * torch.sum(preds == labels_a).item() + (1.0 - lmd) * torch.sum(preds == labels_b).item()\n",
    "                    # else:\n",
    "                    #     outputs = model(inputs)\n",
    "                    #     _, preds = torch.max(outputs, 1)\n",
    "                    #     loss = criterion(outputs, labels)\n",
    "                    #     # 通常の正確さの計算\n",
    "                    #     correct_predictions += torch.sum(preds == labels.data).item()\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # 通常の正確さの計算\n",
    "                    correct_predictions += torch.sum(preds == labels.data).item()\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = correct_predictions / len(dataloaders[phase].dataset)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_auc.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'valid':\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_auc.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'valid':\n",
    "                early_stopping(-epoch_acc, model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    stop_training = True\n",
    "                    break\n",
    "\n",
    "        if stop_training:\n",
    "            break\n",
    "    \n",
    "    # ログを保存\n",
    "    df = pd.DataFrame({'train_loss':train_loss,\n",
    "                      'train_auc':train_auc,\n",
    "                       'val_loss':val_loss,\n",
    "                       'val_auc':val_auc})\n",
    "    df.to_csv(f\"log_output_{notebook_name}_{file_name}.csv\")\n",
    " \n",
    "    print('Finished Training') \n",
    "    \n",
    "    return early_stopping.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a4fdb",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e5cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== train_1  学習中 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 302MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [04:17<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.4398 Acc: 0.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:27<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3550 Acc: 0.8628\n",
      "Validation loss decreased (inf --> -0.862765).  Saving model ...\n",
      "Epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [04:10<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2344 Acc: 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:26<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.2237 Acc: 0.9152\n",
      "Validation loss decreased (-0.862765 --> -0.915237).  Saving model ...\n",
      "Epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [04:10<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1900 Acc: 0.9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:26<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.1121 Acc: 0.9606\n",
      "Validation loss decreased (-0.915237 --> -0.960646).  Saving model ...\n",
      "Epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [04:10<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1652 Acc: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:26<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.1190 Acc: 0.9526\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 37/165 [00:56<03:13,  1.51s/it]"
     ]
    }
   ],
   "source": [
    "eval_dict=dict()\n",
    "\n",
    "for ds in ['train_1','train_2','train_3','train_4']:\n",
    "\n",
    "    print(f'=== {ds}  学習中 ===')\n",
    "    \n",
    "    # Load labels from csv\n",
    "    df = pd.read_csv(f'../data/{ds}.csv', header=None, names=['name', 'label'])\n",
    "    \n",
    "    # 40kB以下の空白画像をdfから削除\n",
    "    file_list = []\n",
    "    for foldername, subfolders, filenames in os.walk(os.path.join(\"../data/\",ds)):\n",
    "        for filename in filenames:\n",
    "            full_path = os.path.join(foldername, filename)\n",
    "            max_size = 40 * 1024\n",
    "            if os.path.getsize(full_path) <= max_size:\n",
    "                file_list.append(filename)\n",
    "    df = df[~df['name'].isin(file_list)]\n",
    "    \n",
    "    # Get transforms\n",
    "    transforms_dict = get_transforms()\n",
    "\n",
    "    # Load datasets\n",
    "    datasets = load_datasets(df, f'../data/{ds}/', transforms_dict)\n",
    "\n",
    "    # Create dataloaders\n",
    "    dataloaders = get_dataloaders(datasets, batch_size=config.batch_size)\n",
    "\n",
    "    # Define model\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2) \n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Train the model\n",
    "    eval_dict[ds]= train_model_mixup_el_stop(\n",
    "        dataloaders,\n",
    "        model, \n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=config.num_epochs,\n",
    "        file_name=ds, \n",
    "        mixup_epochs=config.mixup_epochs,\n",
    "        patience=config.patience\n",
    "    )\n",
    "\n",
    "del df\n",
    "del dataloaders\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea2a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91d7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_ave = sum(eval_dict.values()) / len(eval_dict)\n",
    "acc_ave = round(acc_ave, 4)\n",
    "acc_ave = str(acc_ave)\n",
    "acc_ave = acc_ave.replace('.', '_')\n",
    "\n",
    "print(acc_ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddede80",
   "metadata": {},
   "source": [
    "## 予測フェーズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acdc761",
   "metadata": {},
   "source": [
    "### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_path)  # Use OpenCV to read the image\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        return image, self.image_files[idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb0f43",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_dict = get_transforms()\n",
    "\n",
    "test_data = TestDataset(root_dir='../data/evaluation/', transform=transforms_dict['valid'])\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# モデルのパスをリストにします\n",
    "model_paths = [\n",
    "    f'../models/{formatted_datetime}_train_1.pth',\n",
    "    f'../models/{formatted_datetime}_train_2.pth',\n",
    "    f'../models/{formatted_datetime}_train_3.pth',\n",
    "    f'../models/{formatted_datetime}_train_4.pth',\n",
    "] \n",
    "\n",
    "# 空のリストを作成して各モデルの予測を格納します\n",
    "all_predictions = []\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print('■実行中:', model_path)\n",
    "    \n",
    "    # モデルのインスタンスを作成します（ここではResNet34を使用）\n",
    "    model = torchvision.models.resnet50()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2) \n",
    "\n",
    "    # 状態辞書をロードします\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # モデルをGPUに移動します\n",
    "    model = model.to(device)\n",
    "\n",
    "    # モデルを評価モードに設定します\n",
    "    model.eval()\n",
    "\n",
    "    # このモデルの予測を格納するリストを作成します\n",
    "    model_predictions = []\n",
    "\n",
    "    # データローダーからバッチを取得して予測を行います\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            images, _ = data\n",
    "            images = images.to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            probabilities = F.softmax(logits, dim=1)  # apply softmax to convert logits to probabilities\n",
    "            model_predictions.extend(probabilities.tolist())  # convert PyTorch tensor to Python list\n",
    "\n",
    "    # すべてのモデルの予測を格納するリストにこのモデルの予測を追加します\n",
    "    all_predictions.append(model_predictions)\n",
    "\n",
    "# 全モデルの予測を平均します\n",
    "final_predictions = np.mean(all_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7517aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit用に整形\n",
    "# Create a DataFrame with the image names and the probabilities\n",
    "df_submission = pd.DataFrame({\n",
    "    'image_name': [f'test_{i}.png' for i in range(len(final_predictions))],\n",
    "    'probability': [probs[1] for probs in final_predictions]\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a csv file\n",
    "# df_submission.to_csv(f'../data/submit/submission_{formatted_datetime}_acc{acc_ave}.csv', index=False,header=False)\n",
    "df_submission.to_csv(f'../data/submit/submission_{formatted_datetime}.csv', index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.probability.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.probability.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.probability.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35261963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
